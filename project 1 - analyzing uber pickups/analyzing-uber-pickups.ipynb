{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":793057,"sourceType":"datasetVersion","datasetId":360}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,to_timestamp, hour, dayofweek, when","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T08:24:01.292363Z","iopub.execute_input":"2024-11-29T08:24:01.292757Z","iopub.status.idle":"2024-11-29T08:25:03.798390Z","shell.execute_reply.started":"2024-11-29T08:24:01.292718Z","shell.execute_reply":"2024-11-29T08:25:03.797150Z"}},"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840629 sha256=5f8978961457a659d9b5731dbaf146aaf4a9b601dadc1e8a93aab2b7cbeda631\n  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.3\n","output_type":"stream"},{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/11/29 08:24:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[Stage 1:=============================>                             (2 + 2) / 4]\r","output_type":"stream"},{"name":"stdout","text":"root\n |-- Date/Time: string (nullable = true)\n |-- Lat: double (nullable = true)\n |-- Lon: double (nullable = true)\n |-- Base: string (nullable = true)\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# **initialize spark session**","metadata":{}},{"cell_type":"code","source":"# initialize spark\nspark=SparkSession.builder.appName(\"Uber Data Analysis\").getOrCreate()\n\n# load dataset\nuber_data = spark.read.csv(\"/kaggle/input/uber-pickups-in-new-york-city/uber-raw-data-apr14.csv\", header=True, inferSchema = True)\nuber_data.printSchema()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T09:01:57.603430Z","iopub.execute_input":"2024-11-29T09:01:57.603815Z","iopub.status.idle":"2024-11-29T09:01:58.573286Z","shell.execute_reply.started":"2024-11-29T09:01:57.603781Z","shell.execute_reply":"2024-11-29T09:01:58.572138Z"}},"outputs":[{"name":"stdout","text":"root\n |-- Date/Time: string (nullable = true)\n |-- Lat: double (nullable = true)\n |-- Lon: double (nullable = true)\n |-- Base: string (nullable = true)\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"uber_data.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T09:02:01.262460Z","iopub.execute_input":"2024-11-29T09:02:01.262822Z","iopub.status.idle":"2024-11-29T09:02:01.333509Z","shell.execute_reply.started":"2024-11-29T09:02:01.262789Z","shell.execute_reply":"2024-11-29T09:02:01.332368Z"}},"outputs":[{"name":"stdout","text":"+----------------+-------+--------+------+\n|       Date/Time|    Lat|     Lon|  Base|\n+----------------+-------+--------+------+\n|4/1/2014 0:11:00| 40.769|-73.9549|B02512|\n|4/1/2014 0:17:00|40.7267|-74.0345|B02512|\n|4/1/2014 0:21:00|40.7316|-73.9873|B02512|\n|4/1/2014 0:28:00|40.7588|-73.9776|B02512|\n|4/1/2014 0:33:00|40.7594|-73.9722|B02512|\n|4/1/2014 0:33:00|40.7383|-74.0403|B02512|\n|4/1/2014 0:39:00|40.7223|-73.9887|B02512|\n|4/1/2014 0:45:00| 40.762| -73.979|B02512|\n|4/1/2014 0:55:00|40.7524| -73.996|B02512|\n|4/1/2014 1:01:00|40.7575|-73.9846|B02512|\n|4/1/2014 1:19:00|40.7256|-73.9869|B02512|\n|4/1/2014 1:48:00|40.7591|-73.9684|B02512|\n|4/1/2014 1:49:00|40.7271|-73.9803|B02512|\n|4/1/2014 2:11:00|40.6463|-73.7896|B02512|\n|4/1/2014 2:25:00|40.7564|-73.9167|B02512|\n|4/1/2014 2:31:00|40.7666|-73.9531|B02512|\n|4/1/2014 2:43:00| 40.758|-73.9761|B02512|\n|4/1/2014 3:22:00|40.7238|-73.9821|B02512|\n|4/1/2014 3:35:00|40.7531|-74.0039|B02512|\n|4/1/2014 3:35:00|40.7389|-74.0393|B02512|\n+----------------+-------+--------+------+\nonly showing top 20 rows\n\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"# data cleaning\nuber_data = uber_data.dropDuplicates()\nuber_data = uber_data.na.fill({'Lat':0.0,'Lon':0.0})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T09:02:04.259466Z","iopub.execute_input":"2024-11-29T09:02:04.259851Z","iopub.status.idle":"2024-11-29T09:02:04.275405Z","shell.execute_reply.started":"2024-11-29T09:02:04.259817Z","shell.execute_reply":"2024-11-29T09:02:04.274126Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"# Transformations","metadata":{}},{"cell_type":"code","source":"# transformations\n\n# uber_data = uber_data.withColumn('DateTime',to_timestamp(col('Date/Time'),'MM/dd/yyyy HH:mm:ss'))\n# uber_data = uber_data.withColumn('DateTime',to_timestamp(col('Date/Time'),'M/d/yyyy H:mm:ss'))\n\n'''tried using above code but because Date/Time column \nhas various formatting issues we try to convert this column in a valid format using below code'''\n\n'''rlike is spark sql function that matches strings using regular expression ; similar to like but more\npowerfull'''\n\nuber_data = uber_data.withColumn(\n    \"DateTime\",\n    when(col(\"Date/Time\").rlike(r\"^\\d{1,2}/\\d{1,2}/\\d{4} \\d{1,2}:\\d{2}:\\d{2}$\"),\n         to_timestamp(col(\"Date/Time\"), \"M/d/yyyy H:mm:ss\"))\n    .when(col(\"Date/Time\").rlike(r\"^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}$\"),\n          to_timestamp(col(\"Date/Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n    .otherwise(None)  # Mark invalid rows as None\n)\n\nuber_data = uber_data.withColumn('Hour',hour(col('DateTime')))\nuber_data = uber_data.withColumn('DayOfWeek',dayofweek(col('DateTime')))\nuber_data = uber_data.withColumn('TimeCategory', when((col('Hour')>=5) & (col('Hour')<12), 'Morning').\n                                when((col('Hour')>=12) & (col('Hour')<17), 'Afternoon').\n                                when((col('Hour')>=17) & (col('Hour')<22),'Evening').\n                                otherwise('Night'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T09:02:07.566581Z","iopub.execute_input":"2024-11-29T09:02:07.566965Z","iopub.status.idle":"2024-11-29T09:02:07.635205Z","shell.execute_reply.started":"2024-11-29T09:02:07.566930Z","shell.execute_reply":"2024-11-29T09:02:07.633851Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"uber_data.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T09:02:11.035834Z","iopub.execute_input":"2024-11-29T09:02:11.036654Z","iopub.status.idle":"2024-11-29T09:02:12.926186Z","shell.execute_reply.started":"2024-11-29T09:02:11.036595Z","shell.execute_reply":"2024-11-29T09:02:12.923199Z"}},"outputs":[{"name":"stderr","text":"[Stage 171:==========================================>              (3 + 1) / 4]\r","output_type":"stream"},{"name":"stdout","text":"+-----------------+-------+--------+------+-------------------+----+---------+------------+\n|        Date/Time|    Lat|     Lon|  Base|           DateTime|Hour|DayOfWeek|TimeCategory|\n+-----------------+-------+--------+------+-------------------+----+---------+------------+\n|4/1/2014 12:32:00|40.7191|-73.9973|B02512|2014-04-01 12:32:00|  12|        3|   Afternoon|\n|4/1/2014 17:46:00| 40.753|-73.9701|B02512|2014-04-01 17:46:00|  17|        3|     Evening|\n|4/1/2014 19:05:00|40.7578|-73.9722|B02512|2014-04-01 19:05:00|  19|        3|     Evening|\n|4/1/2014 22:44:00|40.7627|-73.9832|B02512|2014-04-01 22:44:00|  22|        3|       Night|\n| 4/2/2014 7:42:00|40.7707|-73.9632|B02512|2014-04-02 07:42:00|   7|        4|     Morning|\n| 4/2/2014 8:06:00|40.7305|-73.9832|B02512|2014-04-02 08:06:00|   8|        4|     Morning|\n| 4/2/2014 8:11:00|40.7288|-73.9994|B02512|2014-04-02 08:11:00|   8|        4|     Morning|\n|4/2/2014 12:18:00| 40.758|-73.9849|B02512|2014-04-02 12:18:00|  12|        4|   Afternoon|\n|4/2/2014 15:30:00|40.7787|-73.9872|B02512|2014-04-02 15:30:00|  15|        4|   Afternoon|\n|4/2/2014 16:16:00|40.7506|-73.9809|B02512|2014-04-02 16:16:00|  16|        4|   Afternoon|\n|4/2/2014 16:21:00|40.7878| -73.955|B02512|2014-04-02 16:21:00|  16|        4|   Afternoon|\n|4/2/2014 16:23:00|40.7547|-73.9919|B02512|2014-04-02 16:23:00|  16|        4|   Afternoon|\n|4/2/2014 16:50:00|40.7353|-73.9919|B02512|2014-04-02 16:50:00|  16|        4|   Afternoon|\n| 4/3/2014 7:22:00|40.7303| -74.003|B02512|2014-04-03 07:22:00|   7|        5|     Morning|\n|4/3/2014 10:30:00|40.7277|-73.9875|B02512|2014-04-03 10:30:00|  10|        5|     Morning|\n|4/3/2014 12:48:00|40.7311|-74.0089|B02512|2014-04-03 12:48:00|  12|        5|   Afternoon|\n|4/3/2014 16:33:00|40.7282|-74.0078|B02512|2014-04-03 16:33:00|  16|        5|   Afternoon|\n|4/3/2014 16:47:00|40.7238|-73.9983|B02512|2014-04-03 16:47:00|  16|        5|   Afternoon|\n|4/3/2014 17:25:00|40.7209|-74.0029|B02512|2014-04-03 17:25:00|  17|        5|     Evening|\n|4/3/2014 18:01:00|40.7731|-73.8854|B02512|2014-04-03 18:01:00|  18|        5|     Evening|\n+-----------------+-------+--------+------+-------------------+----+---------+------------+\nonly showing top 20 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"# Analysis","metadata":{}},{"cell_type":"code","source":"# Busiest hours for Uber rides\nbusiness_hours = uber_data.groupBy('Hour').count().orderBy(col('count').desc())\nbusiest_hours.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T09:02:51.600974Z","iopub.execute_input":"2024-11-29T09:02:51.601425Z","iopub.status.idle":"2024-11-29T09:02:58.536993Z","shell.execute_reply.started":"2024-11-29T09:02:51.601387Z","shell.execute_reply":"2024-11-29T09:02:58.534267Z"}},"outputs":[{"name":"stderr","text":"[Stage 183:>                                                        (0 + 4) / 4]\r","output_type":"stream"},{"name":"stdout","text":"+----+-----+\n|Hour|count|\n+----+-----+\n|  17|44888|\n|  18|42439|\n|  16|41408|\n|  19|38380|\n|  21|36427|\n|  20|35729|\n|  15|34835|\n|  22|30189|\n|  14|26851|\n|   7|24624|\n|   8|22577|\n|  13|22329|\n|  23|20335|\n|  12|19179|\n|  11|18545|\n|   6|18224|\n|   9|17758|\n|  10|17660|\n|   0|11716|\n|   5| 9302|\n+----+-----+\nonly showing top 20 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# Most popular pickup locations\npopular_locations = uber_data.groupBy('Lat','Lon').count().orderBy(col('count').desc())\npopular_locations.show(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T09:03:03.756858Z","iopub.execute_input":"2024-11-29T09:03:03.757235Z","iopub.status.idle":"2024-11-29T09:03:06.703172Z","shell.execute_reply.started":"2024-11-29T09:03:03.757202Z","shell.execute_reply":"2024-11-29T09:03:06.699808Z"}},"outputs":[{"name":"stderr","text":"[Stage 191:==========================================>              (3 + 1) / 4]\r","output_type":"stream"},{"name":"stdout","text":"+-------+--------+-----+\n|    Lat|     Lon|count|\n+-------+--------+-----+\n|40.6449|-73.7822|  408|\n|40.6449|-73.7821|  369|\n|40.6449|-73.7823|  364|\n| 40.645|-73.7819|  364|\n| 40.645| -73.782|  315|\n|40.7685|-73.8625|  282|\n|40.7741|-73.8725|  233|\n|40.7741|-73.8726|  219|\n|40.6449|-73.7824|  201|\n| 40.774|-73.8726|  189|\n+-------+--------+-----+\nonly showing top 10 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"\n# partition the dataset by Base column \n\n# save the file to working directory of kaggle ; further you can download the file in local machine as well\n","metadata":{}},{"cell_type":"code","source":"# Partition data by 'Base' and write to Parquet\nuber_data.write.partitionBy(\"Base\").parquet(\"/kaggle/working/processed_uber_data.parquet\", mode=\"overwrite\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T08:58:30.486673Z","iopub.execute_input":"2024-11-29T08:58:30.487042Z","iopub.status.idle":"2024-11-29T08:58:39.411921Z","shell.execute_reply.started":"2024-11-29T08:58:30.487011Z","shell.execute_reply":"2024-11-29T08:58:39.410666Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"# why partitioning\n\n* improved query performance - When querying the data, only the partitions relevant to the query are read, reducing the amount of data scanned and improving performance\n\n* efficient & management - Data is split into smaller, more manageable chunks based on the values in the base column, making it easier to handle large datasets\n  \n* Scalability: - Partitioning is especially useful for distributed storage systems (e.g., HDFS, S3), as it enables parallel processing on partitions.\n  \n* Cost-Effectiveness - In cloud environments (e.g., AWS S3, Databricks), partition pruning reduces the cost of processing by scanning fewer partitions.","metadata":{}},{"cell_type":"markdown","source":"# save morning category data in a file","metadata":{}},{"cell_type":"code","source":"# save morning category data in a file\n\nmorning_data = uber_data.filter(col('TimeCategory')=='Morning')\nmorning_data.write.csv('/kaggle/working/morning_data.csv',header=True, mode=\"overwrite\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T09:20:28.443586Z","iopub.execute_input":"2024-11-29T09:20:28.444155Z","iopub.status.idle":"2024-11-29T09:20:34.777085Z","shell.execute_reply.started":"2024-11-29T09:20:28.444116Z","shell.execute_reply":"2024-11-29T09:20:34.776010Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":43}]}